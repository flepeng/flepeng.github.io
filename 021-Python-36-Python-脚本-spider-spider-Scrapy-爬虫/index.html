

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2488174175014870" crossorigin="anonymous"></script><!-- google 广告 -->
  <meta name="google-site-verification" content="40lMg4eqLLbXoDcpN3h-cEnfmselbQ8tUzNvuC0IRIs" /><!-- google 站点认证 -->
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Lepeng">
  <meta name="keywords" content="">
  
    <meta name="description" content="Scrapy 简介Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取)所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 S">
<meta property="og:type" content="article">
<meta property="og:title" content="spider - Scrapy 爬虫">
<meta property="og:url" content="https://flepeng.github.io/021-Python-36-Python-%E8%84%9A%E6%9C%AC-spider-spider-Scrapy-%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="Lepeng">
<meta property="og:description" content="Scrapy 简介Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取)所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 S">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://flepeng.github.io/img/python/spider01.png">
<meta property="article:published_time" content="2016-08-05T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-03T10:25:30.343Z">
<meta property="article:author" content="Feng Lepeng">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://flepeng.github.io/img/python/spider01.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>spider - Scrapy 爬虫 - Lepeng</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"flepeng.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"f3d259b9efd9ce8655c180fd01bf0045","google":{"measurement_id":"G-LFTE4C7W3W"},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?f3d259b9efd9ce8655c180fd01bf0045";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-LFTE4C7W3W", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', 'G-LFTE4C7W3W');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Lepeng 的 blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="spider - Scrapy 爬虫"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2016-08-06 00:00" pubdate>
          2016年8月6日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.1k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          43 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">spider - Scrapy 爬虫</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="Scrapy-简介"><a href="#Scrapy-简介" class="headerlink" title="Scrapy 简介"></a>Scrapy 简介</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取)所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img src="/img/python/spider01.png" srcset="/img/loading.gif" lazyload></p>
<p>Scrapy主要包括了以下组件：</p>
<ul>
<li><p><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)</p>
</li>
<li><p><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</p>
</li>
<li><p><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</p>
</li>
<li><p><strong>爬虫(Spiders)</strong><br>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</p>
</li>
<li><p><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</p>
</li>
<li><p><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</p>
</li>
<li><p><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</p>
</li>
<li><p><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</p>
</li>
</ul>
<p>Scrapy运行流程大概如下：</p>
<ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p>Linux</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip3 <span class="hljs-keyword">install</span> scrapy<br></code></pre></td></tr></table></figure>

<p>Windows</p>
<ol>
<li>pip3 install wheel</li>
<li>下载twisted <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></li>
<li>进入下载目录，执行 pip3 install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</li>
<li>pip3 install scrapy</li>
<li>下载并安装pywin32：<a href="https://sourceforge.net/projects/pywin32/files/" target="_blank" rel="noopener">https://sourceforge.net/projects/pywin32/files/</a></li>
</ol>
<h2 id="二、基本使用"><a href="#二、基本使用" class="headerlink" title="二、基本使用"></a>二、基本使用</h2><h3 id="1-基本命令"><a href="#1-基本命令" class="headerlink" title="1. 基本命令"></a>1. 基本命令</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>. <span class="hljs-keyword">scrapy </span>startproject 项目名称<br>   <span class="hljs-comment"># 在当前目录中创建一个项目文件（类似于Django）</span><br> <br><span class="hljs-number">2</span>. <span class="hljs-keyword">scrapy </span>genspider [-t template] &lt;name&gt; &lt;domain&gt;<br>   <span class="hljs-comment"># 创建爬虫应用</span><br>      <span class="hljs-keyword">scrapy </span>gensipider -t <span class="hljs-keyword">basic </span>oldboy oldboy.com<br>      <span class="hljs-keyword">scrapy </span>gensipider -t xmlfeed autohome autohome.com.cn<br> <br><span class="hljs-symbol">   PS:</span><br>      查看所有命令：<span class="hljs-keyword">scrapy </span>gensipider -l<br>      查看模板命令：<span class="hljs-keyword">scrapy </span>gensipider -d 模板名称<br> <br><span class="hljs-number">3</span>. <span class="hljs-keyword">scrapy </span>list<br>   <span class="hljs-comment"># 展示爬虫应用列表</span><br> <br><span class="hljs-number">4</span>. <span class="hljs-keyword">scrapy </span>crawl 爬虫应用名称<br>   <span class="hljs-comment"># 运行单独爬虫应用，要在项目内运行</span><br></code></pre></td></tr></table></figure>


<h3 id="2-项目结构以及爬虫应用简介"><a href="#2-项目结构以及爬虫应用简介" class="headerlink" title="2.项目结构以及爬虫应用简介"></a>2.项目结构以及爬虫应用简介</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs r">project_name/<br>   scrapy.cfg         <span class="hljs-comment"># 项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</span><br>   project_name/<br>       __init__.py<br>       items.py       <span class="hljs-comment"># 设置数据存储模板，用于结构化数据，如：Django的Model</span><br>       pipelines.py   <span class="hljs-comment"># 数据处理行为，如：一般结构化的数据持久化</span><br>       settings.py    <span class="hljs-comment"># 配置文件，如：递归的层数、并发数，延迟下载等</span><br>       spiders/       <span class="hljs-comment"># 爬虫目录，如：创建文件，编写爬虫规则</span><br>           __init__.py<br>           爬虫<span class="hljs-number">1.</span>py<br>           爬虫<span class="hljs-number">2.</span>py<br>           爬虫<span class="hljs-number">3.</span>py<br></code></pre></td></tr></table></figure>

<p><em>注意：一般创建爬虫文件时，以网站域名命名</em></p>
<p>爬虫1.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XiaoHuarSpider</span><span class="hljs-params">(scrapy.spiders.Spider)</span>:</span><br>    name = <span class="hljs-string">"spidername"</span>                 <span class="hljs-comment"># 爬虫名称 *****</span><br>    allowed_domains = [<span class="hljs-string">"spider.com"</span>]    <span class="hljs-comment"># 允许的域名</span><br>    start_urls = [<br>        <span class="hljs-string">"http://www.flepeng.com/"</span>,      <span class="hljs-comment"># 起始URL</span><br>    ]<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span><br>        <span class="hljs-comment"># 访问起始URL并获取结果后的回调函数</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<p>关于windows编码</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import sys,os<br>sys.stdout=io.<span class="hljs-constructor">TextIOWrapper(<span class="hljs-params">sys</span>.<span class="hljs-params">stdout</span>.<span class="hljs-params">buffer</span>,<span class="hljs-params">encoding</span>='<span class="hljs-params">gb18030</span>')</span><br></code></pre></td></tr></table></figure>


<h3 id="3-小试牛刀"><a href="#3-小试牛刀" class="headerlink" title="3.小试牛刀"></a>3.小试牛刀</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import scrapy<br><span class="hljs-keyword">from</span> scrapy.selector import HtmlXPathSelector  # 新版的好像已经弃用，使用Selector<br><span class="hljs-keyword">from</span> scrapy.http.request import Request<br> <br> <br>class DigSpider(scrapy.Spider):<br>    name = <span class="hljs-string">"dig"</span>    # 爬虫应用的名称，通过命令启动爬虫时，使用此参数<br>    allowed_domains = [<span class="hljs-string">"chouti.com"</span>]    # 允许的域名<br>    start_urls = [<span class="hljs-string">'http://dig.chouti.com/'</span>,]    # 起始URL<br> <br>    has_request_set = &#123;&#125;<br> <br>    def parse(self, response):<br>        # 访问首页之后的 解析函数<br>        <span class="hljs-builtin-name">print</span>(response.url)<br>        hxs = HtmlXPathSelector(response)<br>        page_list = hxs.select(<span class="hljs-string">'//div[@id="dig_lcpage"]//a[re:test(@href, "/all/hot/recent/\d+")]/@href'</span>).extract()<br>        <span class="hljs-keyword">for</span><span class="hljs-built_in"> page </span><span class="hljs-keyword">in</span> page_list:<br>            page_url = <span class="hljs-string">'http://dig.chouti.com%s'</span> % page<br>            key = self.md5(page_url)<br> <br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.has_request_set:<br>                self.has_request_set[key] = page_url<br>                obj = Request(<span class="hljs-attribute">url</span>=page_url, <span class="hljs-attribute">method</span>=<span class="hljs-string">'GET'</span>, <span class="hljs-attribute">callback</span>=self.parse)  # callback 回调函数<br>                yield obj<br> <br>    @staticmethod<br>    def md5(val):<br>        import hashlib<br>        ha = hashlib.md5()<br>        ha.update(bytes(val, <span class="hljs-attribute">encoding</span>=<span class="hljs-string">'utf-8'</span>))<br>        key = ha.hexdigest()<br>        return key<br></code></pre></td></tr></table></figure>

<p>执行此爬虫文件，则在终端进入项目目录执行如下命令：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs r">scrapy crawl dig --nolog <span class="hljs-comment"># nolog 表示不打印日志</span><br></code></pre></td></tr></table></figure>

<p>对于上述代码重要之处在于：</p>
<ul>
<li>Request是一个封装用户请求的类，在回调函数中yield该对象表示继续访问</li>
<li>HtmlXpathSelector用于结构化HTML代码并提供选择器功能</li>
</ul>
<h3 id="4-选择器"><a href="#4-选择器" class="headerlink" title="4. 选择器"></a>4. 选择器</h3><p>xpath的路径表达式：</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>nodename</td>
<td>选取此节点的所有子节点。</td>
</tr>
<tr>
<td>&#x2F;</td>
<td>从根节点选取。</td>
</tr>
<tr>
<td>&#x2F;&#x2F;</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点。</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点。</td>
</tr>
<tr>
<td>@</td>
<td>选取属性。</td>
</tr>
</tbody></table>
<p>在下面的表格中，列出了一些路径表达式以及表达式的结果：</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>bookstore</td>
<td>选取 bookstore 元素的所有子节点。</td>
</tr>
<tr>
<td>&#x2F;bookstore</td>
<td>选取根元素 bookstore。注释：假如路径起始于正斜杠( &#x2F; )，则此路径始终代表到某元素的绝对路径！</td>
</tr>
<tr>
<td>bookstore&#x2F;book</td>
<td>选取属于 bookstore 的子元素的所有 book 元素。</td>
</tr>
<tr>
<td>&#x2F;&#x2F;book</td>
<td>选取所有 book 子元素，而不管它们在文档中的位置。</td>
</tr>
<tr>
<td>bookstore&#x2F;&#x2F;book</td>
<td>选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td>
</tr>
<tr>
<td>&#x2F;&#x2F;@lang</td>
<td>选取名为 lang 的所有属性。</td>
</tr>
</tbody></table>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br>from scrapy.selector import Selector, HtmlXPathSelector <span class="hljs-comment"># 新版好像已弃，使用Selector，用法和这个一样</span><br>from scrapy.http import HtmlResponse<br><br><br>html = <span class="hljs-string">""</span><span class="hljs-string">"&lt;!DOCTYPE html&gt;</span><br><span class="hljs-string">&lt;html&gt;</span><br><span class="hljs-string">    &lt;head lang="</span>en<span class="hljs-string">"&gt;</span><br><span class="hljs-string">        &lt;meta charset="</span>UTF-<span class="hljs-number">8</span><span class="hljs-string">"&gt;</span><br><span class="hljs-string">        &lt;title&gt;&lt;/title&gt;</span><br><span class="hljs-string">    &lt;/head&gt;</span><br><span class="hljs-string">    &lt;body&gt;</span><br><span class="hljs-string">        &lt;ul&gt;</span><br><span class="hljs-string">            &lt;li class="</span>item-<span class="hljs-string">"&gt;&lt;a id='i1' href="</span>link.html<span class="hljs-string">"&gt;first item&lt;/a&gt;&lt;/li&gt;</span><br><span class="hljs-string">            &lt;li class="</span>item-<span class="hljs-number">0</span><span class="hljs-string">"&gt;&lt;a id='i2' href="</span>llink.html<span class="hljs-string">"&gt;first item&lt;/a&gt;&lt;/li&gt;</span><br><span class="hljs-string">            &lt;li class="</span>item-<span class="hljs-number">1</span><span class="hljs-string">"&gt;&lt;a href="</span>llink2.html<span class="hljs-string">"&gt;second item&lt;span&gt;vv&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="hljs-string">        &lt;/ul&gt;</span><br><span class="hljs-string">        &lt;div&gt;&lt;a href="</span>llink2.html<span class="hljs-string">"&gt;second item&lt;/a&gt;&lt;/div&gt;</span><br><span class="hljs-string">    &lt;/body&gt;</span><br><span class="hljs-string">&lt;/html&gt;</span><br><span class="hljs-string">"</span><span class="hljs-string">""</span><br> <br>response = HtmlResponse(url=<span class="hljs-string">'http://example.com'</span>, body=html,encoding=<span class="hljs-string">'utf-8'</span>)<br><span class="hljs-comment"># hxs = HtmlXPathSelector(response)</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a')  # 从根目录下查找所有 a 元素</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[2]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[<span class="hljs-doctag">@id</span>]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[<span class="hljs-doctag">@id</span>="i1"]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[<span class="hljs-doctag">@href</span>="link.html"][<span class="hljs-doctag">@id</span>="i1"]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[contains(<span class="hljs-doctag">@href</span>, "link")]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[starts-with(<span class="hljs-doctag">@href</span>, "link")]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[re:test(<span class="hljs-doctag">@id</span>, "i\d+")]')</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[re:test(<span class="hljs-doctag">@id</span>, "i\d+")]/text()').extract()</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//a[re:test(<span class="hljs-doctag">@id</span>, "i\d+")]/<span class="hljs-doctag">@href</span>').extract()</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('/html/body/ul/li/a/<span class="hljs-doctag">@href</span>').extract()</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># hxs = Selector(response=response).xpath('//body/ul/li/a/<span class="hljs-doctag">@href</span>').extract_first()</span><br><span class="hljs-comment"># print(hxs)</span><br><span class="hljs-comment"># ul_list = Selector(response=response).xpath('//body/ul/li')</span><br><span class="hljs-comment"># for item in ul_list:</span><br><span class="hljs-comment">#     v = item.xpath('./a/span')</span><br><span class="hljs-comment">#     # 或</span><br><span class="hljs-comment">#     # v = item.xpath('a/span')</span><br><span class="hljs-comment">#     # 或</span><br><span class="hljs-comment">#     # v = item.xpath('*/a/span')</span><br><span class="hljs-comment">#     print(v)</span><br></code></pre></td></tr></table></figure>

<p>示例：自动登陆抽屉并点赞</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br>import scrapy<br><span class="hljs-keyword">from</span> scrapy.selector import HtmlXPathSelector<br><span class="hljs-keyword">from</span> scrapy.http.request import Request<br><span class="hljs-keyword">from</span> scrapy.http.cookies import CookieJar<br><span class="hljs-keyword">from</span> scrapy import FormRequest<br> <br> <br>class ChouTiSpider(scrapy.Spider):<br>    # 爬虫应用的名称，通过此名称启动爬虫命令<br>    name = <span class="hljs-string">"chouti"</span><br>    # 允许的域名<br>    allowed_domains = [<span class="hljs-string">"chouti.com"</span>]<br> <br>    cookie_dict = &#123;&#125;<br>    has_request_set = &#123;&#125;<br> <br>    def start_requests(self):<br>        url = <span class="hljs-string">'http://dig.chouti.com/'</span><br>        # return [Request(<span class="hljs-attribute">url</span>=url, <span class="hljs-attribute">callback</span>=self.login)]<br>        yield Request(<span class="hljs-attribute">url</span>=url, <span class="hljs-attribute">callback</span>=self.login)<br> <br>    def login(self, response):<br>        cookie_jar = CookieJar()<br>        cookie_jar.extract_cookies(response, response.request)<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> cookie_jar._cookies.items():<br>            <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> v.items():<br>                <span class="hljs-keyword">for</span> m, n <span class="hljs-keyword">in</span> j.items():<br>                    self.cookie_dict[m] = n.value<br> <br>        req = Request(<br>            <span class="hljs-attribute">url</span>=<span class="hljs-string">'http://dig.chouti.com/login'</span>,<br>            <span class="hljs-attribute">method</span>=<span class="hljs-string">'POST'</span>,<br>            headers=&#123;<span class="hljs-string">'Content-Type'</span>: <span class="hljs-string">'application/x-www-form-urlencoded; charset=UTF-8'</span>&#125;,<br>            <span class="hljs-attribute">body</span>=<span class="hljs-string">'phone=8615131255089&amp;password=pppppppp&amp;oneMonth=1'</span>,<br>            <span class="hljs-attribute">cookies</span>=self.cookie_dict,<br>            <span class="hljs-attribute">callback</span>=self.check_login<br>        )<br>        yield req<br> <br>    def check_login(self, response):<br>        req = Request(<br>            <span class="hljs-attribute">url</span>=<span class="hljs-string">'http://dig.chouti.com/'</span>,<br>            <span class="hljs-attribute">method</span>=<span class="hljs-string">'GET'</span>,<br>            <span class="hljs-attribute">callback</span>=self.show,<br>            <span class="hljs-attribute">cookies</span>=self.cookie_dict,<br>            <span class="hljs-attribute">dont_filter</span>=<span class="hljs-literal">True</span><br>        )<br>        yield req<br> <br>    def show(self, response):<br>        # <span class="hljs-builtin-name">print</span>(response)<br>        hxs = HtmlXPathSelector(response)<br>        news_list = hxs.select(<span class="hljs-string">'//div[@id="content-list"]/div[@class="item"]'</span>)<br>        <span class="hljs-keyword">for</span> new <span class="hljs-keyword">in</span> news_list:<br>            # temp = new.xpath(<span class="hljs-string">'div/div[@class="part2"]/@share-linkid'</span>).extract()<br>            link_id = new.xpath(<span class="hljs-string">'*/div[@class="part2"]/@share-linkid'</span>).extract_first()<br>            yield Request(<br>                <span class="hljs-attribute">url</span>=<span class="hljs-string">'http://dig.chouti.com/link/vote?linksId=%s'</span> %(link_id,),<br>                <span class="hljs-attribute">method</span>=<span class="hljs-string">'POST'</span>,<br>                <span class="hljs-attribute">cookies</span>=self.cookie_dict,<br>                <span class="hljs-attribute">callback</span>=self.do_favor<br>            )<br> <br>        page_list = hxs.select(<span class="hljs-string">'//div[@id="dig_lcpage"]//a[re:test(@href, "/all/hot/recent/\d+")]/@href'</span>).extract()<br>        <span class="hljs-keyword">for</span><span class="hljs-built_in"> page </span><span class="hljs-keyword">in</span> page_list:<br> <br>            page_url = <span class="hljs-string">'http://dig.chouti.com%s'</span> % page<br>            import hashlib<br>            hash = hashlib.md5()<br>            hash.update(bytes(page_url,<span class="hljs-attribute">encoding</span>=<span class="hljs-string">'utf-8'</span>))<br>            key = hash.hexdigest()<br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> self.has_request_set:<br>                pass<br>            <span class="hljs-keyword">else</span>:<br>                self.has_request_set[key] = page_url<br>                yield Request(<br>                    <span class="hljs-attribute">url</span>=page_url,<br>                    <span class="hljs-attribute">method</span>=<span class="hljs-string">'GET'</span>,<br>                    <span class="hljs-attribute">callback</span>=self.show<br>                )<br> <br>    def do_favor(self, response):<br>        <span class="hljs-builtin-name">print</span>(response.text)<br></code></pre></td></tr></table></figure>

<p>处理Cookie</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> scrapy.http.response.html <span class="hljs-keyword">import</span> HtmlResponse<br><span class="hljs-keyword">from</span> scrapy.http <span class="hljs-keyword">import</span> Request<br><span class="hljs-keyword">from</span> scrapy.http.cookies <span class="hljs-keyword">import</span> CookieJar<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ChoutiSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span><br>    name = <span class="hljs-string">"chouti"</span><br>    allowed_domains = [<span class="hljs-string">"chouti.com"</span>]<br>    start_urls = (<span class="hljs-string">'http://www.chouti.com/'</span>,)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start_requests</span><span class="hljs-params">(self)</span>:</span><br>        url = <span class="hljs-string">'http://dig.chouti.com/'</span><br>        <span class="hljs-keyword">yield</span> Request(url=url, callback=self.login, meta=&#123;<span class="hljs-string">'cookiejar'</span>: <span class="hljs-literal">True</span>&#125;)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">login</span><span class="hljs-params">(self, response)</span>:</span><br>        print(response.headers.getlist(<span class="hljs-string">'Set-Cookie'</span>))<br>        req = Request(<br>            url=<span class="hljs-string">'http://dig.chouti.com/login'</span>,<br>            method=<span class="hljs-string">'POST'</span>,<br>            headers=&#123;<span class="hljs-string">'Content-Type'</span>: <span class="hljs-string">'application/x-www-form-urlencoded; charset=UTF-8'</span>&#125;,<br>            body=<span class="hljs-string">'phone=8613121758648&amp;password=woshiniba&amp;oneMonth=1'</span>,<br>            callback=self.check_login,<br>            meta=&#123;<span class="hljs-string">'cookiejar'</span>: <span class="hljs-literal">True</span>&#125;<br>        )<br>        <span class="hljs-keyword">yield</span> req<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">check_login</span><span class="hljs-params">(self, response)</span>:</span><br>        print(response.text)<br></code></pre></td></tr></table></figure>

<p><em>注意：settings.py中设置DEPTH_LIMIT &#x3D; 1来指定“递归”的层数。</em></p>
<h3 id="5-格式化处理-pipelines"><a href="#5-格式化处理-pipelines" class="headerlink" title="5. 格式化处理 pipelines"></a>5. 格式化处理 pipelines</h3><p>上述实例只是简单的处理，所以在parse方法中直接处理。如果对于想要获取更多的数据处理，则可以利用Scrapy的items将数据格式化，然后统一交由pipelines来处理。</p>
<p>spiders&#x2F;xiahuar.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> scrapy.selector <span class="hljs-keyword">import</span> HtmlXPathSelector<br><span class="hljs-keyword">from</span> scrapy.http.request <span class="hljs-keyword">import</span> Request<br><span class="hljs-keyword">from</span> scrapy.http.cookies <span class="hljs-keyword">import</span> CookieJar<br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> FormRequest<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XiaoHuarSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span><br>    name = <span class="hljs-string">"xiaohuar"</span><br>    allowed_domains = [<span class="hljs-string">"xiaohuar.com"</span>]<br> <br>    start_urls = [<span class="hljs-string">"http://www.xiaohuar.com/list-1-1.html"</span>,]<br>    <span class="hljs-comment"># setting 中的配置pipelines</span><br>    <span class="hljs-comment"># custom_settings = &#123;</span><br>    <span class="hljs-comment">#     'ITEM_PIPELINES':&#123;</span><br>    <span class="hljs-comment">#         'spider1.pipelines.JsonPipeline': 100</span><br>    <span class="hljs-comment">#     &#125;</span><br>    <span class="hljs-comment"># &#125;</span><br>    has_request_set = &#123;&#125;<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span><br>        <span class="hljs-comment"># 分析页面</span><br>        <span class="hljs-comment"># 找到页面中符合规则的内容（校花图片），保存</span><br>        <span class="hljs-comment"># 找到所有的a标签，再访问其他a标签，一层一层的搞下去</span><br> <br>        hxs = HtmlXPathSelector(response)<br> <br>        items = hxs.select(<span class="hljs-string">'//div[@class="item_list infinite_scroll"]/div'</span>)<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items:<br>            src = item.select(<span class="hljs-string">'.//div[@class="img"]/a/img/@src'</span>).extract_first()<br>            name = item.select(<span class="hljs-string">'.//div[@class="img"]/span/text()'</span>).extract_first()<br>            school = item.select(<span class="hljs-string">'.//div[@class="img"]/div[@class="btns"]/a/text()'</span>).extract_first()<br>            url = <span class="hljs-string">"http://www.xiaohuar.com%s"</span> % src<br>            <span class="hljs-keyword">from</span> ..items <span class="hljs-keyword">import</span> XiaoHuarItem<br>            obj = XiaoHuarItem(name=name, school=school, url=url)<br>            <span class="hljs-keyword">yield</span> obj<br> <br>        urls = hxs.select(<span class="hljs-string">'//a[re:test(@href, "http://www.xiaohuar.com/list-1-\d+.html")]/@href'</span>)<br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls:<br>            key = self.md5(url)<br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> self.has_request_set:<br>                <span class="hljs-keyword">pass</span><br>            <span class="hljs-keyword">else</span>:<br>                self.has_request_set[key] = url<br>                req = Request(url=url,method=<span class="hljs-string">'GET'</span>,callback=self.parse)<br>                <span class="hljs-keyword">yield</span> req<br> <br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">md5</span><span class="hljs-params">(val)</span>:</span><br>        <span class="hljs-keyword">import</span> hashlib<br>        ha = hashlib.md5()<br>        ha.update(bytes(val, encoding=<span class="hljs-string">'utf-8'</span>))<br>        key = ha.hexdigest()<br>        <span class="hljs-keyword">return</span> key<br></code></pre></td></tr></table></figure>

<p>items</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XiaoHuarItem</span><span class="hljs-params">(scrapy.Item)</span>:</span><br>    name = scrapy.Field()<br>    school = scrapy.Field()<br>    url = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>pipelines</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> requests<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">JsonPipeline</span><span class="hljs-params">(object)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span><br>        self.file = open(<span class="hljs-string">'xiaohua.txt'</span>, <span class="hljs-string">'w'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span><br>        v = json.dumps(dict(item), ensure_ascii=<span class="hljs-literal">False</span>)<br>        self.file.write(v)<br>        self.file.write(<span class="hljs-string">'\n'</span>)<br>        self.file.flush()<br>        <span class="hljs-keyword">return</span> item<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FilePipeline</span><span class="hljs-params">(object)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">'imgs'</span>):<br>            os.makedirs(<span class="hljs-string">'imgs'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span><br>        response = requests.get(item[<span class="hljs-string">'url'</span>], stream=<span class="hljs-literal">True</span>)<br>        file_name = <span class="hljs-string">'%s_%s.jpg'</span> % (item[<span class="hljs-string">'name'</span>], item[<span class="hljs-string">'school'</span>])<br>        <span class="hljs-keyword">with</span> open(os.path.join(<span class="hljs-string">'imgs'</span>, file_name), mode=<span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:<br>            f.write(response.content)<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure>

<p>settings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">'spider1.pipelines.JsonPipeline'</span>: <span class="hljs-number">100</span>,<br>   <span class="hljs-string">'spider1.pipelines.FilePipeline'</span>: <span class="hljs-number">300</span>,<br>&#125;<br><span class="hljs-comment"># 后面的整数值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。</span><br></code></pre></td></tr></table></figure>

<p>对于pipeline可以做更多，如下：</p>
<p>自定义pipeline格式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy.exceptions <span class="hljs-keyword">import</span> DropItem<br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomPipeline</span><span class="hljs-params">(object)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,v)</span>:</span><br>        self.value = v<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span><br>        <span class="hljs-comment"># 运行pipeline时会调用此函数，操作并进行持久化</span><br>        <span class="hljs-comment"># return表示会被后续的pipeline继续处理</span><br>        <span class="hljs-keyword">return</span> item<br> <br>        <span class="hljs-comment"># 表示将item丢弃，不会被后续pipeline处理</span><br>        <span class="hljs-comment"># raise DropItem()</span><br> <br> <br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">from_crawler</span><span class="hljs-params">(cls, crawler)</span>:</span><br>        <span class="hljs-comment"># 初始化时候，用于创建pipeline对象</span><br>        val = crawler.settings.getint(<span class="hljs-string">'MMMM'</span>)<br>        <span class="hljs-keyword">return</span> cls(val)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">open_spider</span><span class="hljs-params">(self,spider)</span>:</span><br>        <span class="hljs-comment"># 爬虫开始执行时，调用</span><br>        print(<span class="hljs-string">'000000'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close_spider</span><span class="hljs-params">(self,spider)</span>:</span><br>        <span class="hljs-comment"># 爬虫关闭时，被调用</span><br>        print(<span class="hljs-string">'111111'</span>)<br></code></pre></td></tr></table></figure>


<h3 id="6-中间件"><a href="#6-中间件" class="headerlink" title="6.中间件"></a>6.中间件</h3><h4 id="爬虫中间件"><a href="#爬虫中间件" class="headerlink" title="爬虫中间件"></a>爬虫中间件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SpiderMiddleware</span><span class="hljs-params">(object)</span>:</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_spider_input</span><span class="hljs-params">(self,response, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        下载完成，执行，然后交给parse处理</span><br><span class="hljs-string">        :param response: </span><br><span class="hljs-string">        :param spider: </span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">pass</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_spider_output</span><span class="hljs-params">(self,response, result, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        spider处理完成，返回时调用</span><br><span class="hljs-string">        :param response:</span><br><span class="hljs-string">        :param result:</span><br><span class="hljs-string">        :param spider:</span><br><span class="hljs-string">        :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> result<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_spider_exception</span><span class="hljs-params">(self,response, exception, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        异常调用</span><br><span class="hljs-string">        :param response:</span><br><span class="hljs-string">        :param exception:</span><br><span class="hljs-string">        :param spider:</span><br><span class="hljs-string">        :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br> <br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_start_requests</span><span class="hljs-params">(self,start_requests, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        爬虫启动时调用</span><br><span class="hljs-string">        :param start_requests:</span><br><span class="hljs-string">        :param spider:</span><br><span class="hljs-string">        :return: 包含 Request 对象的可迭代对象</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> start_requests<br></code></pre></td></tr></table></figure>

<h4 id="下载器中间件"><a href="#下载器中间件" class="headerlink" title="下载器中间件"></a>下载器中间件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DownMiddleware1</span><span class="hljs-params">(object)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_request</span><span class="hljs-params">(self, request, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span><br><span class="hljs-string">        :param request: </span><br><span class="hljs-string">        :param spider: </span><br><span class="hljs-string">        :return:  </span><br><span class="hljs-string">            None,继续后续中间件去下载；</span><br><span class="hljs-string">            Response对象，停止process_request的执行，开始执行process_response</span><br><span class="hljs-string">            Request对象，停止中间件的执行，将Request重新调度器</span><br><span class="hljs-string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">pass</span><br> <br> <br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_response</span><span class="hljs-params">(self, request, response, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        spider处理完成，返回时调用</span><br><span class="hljs-string">        :param response:</span><br><span class="hljs-string">        :param result:</span><br><span class="hljs-string">        :param spider:</span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">            Response 对象：转交给其他中间件process_response</span><br><span class="hljs-string">            Request 对象：停止中间件，request会被重新调度下载</span><br><span class="hljs-string">            raise IgnoreRequest 异常：调用Request.errback</span><br><span class="hljs-string">        """</span><br>        print(<span class="hljs-string">'response1'</span>)<br>        <span class="hljs-keyword">return</span> response<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_exception</span><span class="hljs-params">(self, request, exception, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span><br><span class="hljs-string">        :param response:</span><br><span class="hljs-string">        :param exception:</span><br><span class="hljs-string">        :param spider:</span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">            None：继续交给后续中间件处理异常；</span><br><span class="hljs-string">            Response对象：停止后续process_exception方法</span><br><span class="hljs-string">            Request对象：停止中间件，request将会被重新调用下载</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>


<h3 id="7-自定制命令"><a href="#7-自定制命令" class="headerlink" title="7. 自定制命令"></a>7. 自定制命令</h3><ul>
<li>在spiders同级创建任意目录，如：commands</li>
<li>在其中创建 crawlall.py 文件 （此处文件名就是自定义的命令）</li>
</ul>
<p>crawlall.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy.commands <span class="hljs-keyword">import</span> ScrapyCommand<br><span class="hljs-keyword">from</span> scrapy.utils.project <span class="hljs-keyword">import</span> get_project_settings<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Command</span><span class="hljs-params">(ScrapyCommand)</span>:</span><br>    requires_project = <span class="hljs-literal">True</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">syntax</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 命令的参数</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">'[options]'</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">short_desc</span><span class="hljs-params">(self)</span>:</span>    <span class="hljs-comment"># 命令的描述</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">'Runs all of the spiders'</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">(self, args, opts)</span>:</span><br>        spider_list = self.crawler_process.spiders.list()<br>        <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> spider_list:<br>            self.crawler_process.crawl(name, **opts.__dict__)<br>        self.crawler_process.start()<br></code></pre></td></tr></table></figure>

<ul>
<li>在settings.py 中添加配置 COMMANDS_MODULE &#x3D; ‘项目名称.目录名称’</li>
<li>在项目目录执行命令：scrapy crawlall</li>
</ul>
<p>单个爬虫</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs smali">import sys<br>from scrapy.cmdline import execute<br> <br>if __name__ == '__main__':<br>    execute([<span class="hljs-string">"scrapy"</span>,<span class="hljs-string">"github"</span>,<span class="hljs-string">"--nolog"</span>])<br></code></pre></td></tr></table></figure>


<h3 id="8-自定义扩展"><a href="#8-自定义扩展" class="headerlink" title="8. 自定义扩展"></a>8. 自定义扩展</h3><p>自定义扩展时，利用信号在指定位置注册制定操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyExtension</span><span class="hljs-params">(object)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, value)</span>:</span><br>        self.value = value<br> <br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">from_crawler</span><span class="hljs-params">(cls, crawler)</span>:</span><br>        val = crawler.settings.getint(<span class="hljs-string">'MMMM'</span>)<br>        ext = cls(val)<br> <br>        <span class="hljs-comment"># 注册信号</span><br>        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)<br>        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)<br> <br>        <span class="hljs-keyword">return</span> ext<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">spider_opened</span><span class="hljs-params">(self, spider)</span>:</span><br>        print(<span class="hljs-string">'open'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">spider_closed</span><span class="hljs-params">(self, spider)</span>:</span><br>        print(<span class="hljs-string">'close'</span>)<br></code></pre></td></tr></table></figure>


<h3 id="9-避免重复访问"><a href="#9-避免重复访问" class="headerlink" title="9. 避免重复访问"></a>9. 避免重复访问</h3><p>scrapy默认使用 scrapy.dupefilter.RFPDupeFilter 进行去重，相关配置有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">DUPEFILTER_CLASS = <span class="hljs-string">'scrapy.dupefilter.RFPDupeFilter'</span><br>DUPEFILTER_DEBUG = <span class="hljs-literal">False</span><br>JOBDIR = <span class="hljs-string">"保存范文记录的日志路径，如：/root/"</span>  <span class="hljs-comment"># 最终路径为 /root/requests.seen</span><br></code></pre></td></tr></table></figure>

<p>自定义URL去重操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RepeatUrl</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span><br>        self.visited_url = set()<br> <br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">from_settings</span><span class="hljs-params">(cls, settings)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        初始化时，调用</span><br><span class="hljs-string">        :param settings: </span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> cls()<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">request_seen</span><span class="hljs-params">(self, request)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        检测当前请求是否已经被访问过</span><br><span class="hljs-string">        :param request: </span><br><span class="hljs-string">        :return: True表示已经访问过；False表示未访问过</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">if</span> request.url <span class="hljs-keyword">in</span> self.visited_url:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        self.visited_url.add(request.url)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">open</span><span class="hljs-params">(self)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        开始爬取请求时，调用</span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">        """</span><br>        print(<span class="hljs-string">'open replication'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close</span><span class="hljs-params">(self, reason)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        结束爬虫爬取时，调用</span><br><span class="hljs-string">        :param reason: </span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">        """</span><br>        print(<span class="hljs-string">'close replication'</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log</span><span class="hljs-params">(self, request, spider)</span>:</span><br>        <span class="hljs-string">"""</span><br><span class="hljs-string">        记录日志</span><br><span class="hljs-string">        :param request: </span><br><span class="hljs-string">        :param spider: </span><br><span class="hljs-string">        :return: </span><br><span class="hljs-string">        """</span><br>        print(<span class="hljs-string">'repeat'</span>, request.url)<br></code></pre></td></tr></table></figure>


<h3 id="10-其他"><a href="#10-其他" class="headerlink" title="10.其他"></a>10.其他</h3><p>settings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br> <br><span class="hljs-comment"># Scrapy settings for step8_king project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     http://doc.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br> <br><span class="hljs-comment"># 1. 爬虫名称</span><br>BOT_NAME = <span class="hljs-string">'step8_king'</span><br> <br><span class="hljs-comment"># 2. 爬虫应用路径</span><br>SPIDER_MODULES = [<span class="hljs-string">'step8_king.spiders'</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">'step8_king.spiders'</span><br> <br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br><span class="hljs-comment"># 3. 客户端 user-agent请求头</span><br><span class="hljs-comment"># USER_AGENT = 'step8_king (+http://www.yourdomain.com)'</span><br> <br><span class="hljs-comment"># Obey robots.txt rules</span><br><span class="hljs-comment"># 4. 禁止爬虫配置，应该开启，看看是否允许</span><br><span class="hljs-comment"># ROBOTSTXT_OBEY = False</span><br> <br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment"># 5. 并发请求数</span><br><span class="hljs-comment"># CONCURRENT_REQUESTS = 4</span><br> <br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment"># 6. 延迟下载秒数</span><br><span class="hljs-comment"># DOWNLOAD_DELAY = 2</span><br> <br> <br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment"># 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名</span><br><span class="hljs-comment"># CONCURRENT_REQUESTS_PER_DOMAIN = 2</span><br><span class="hljs-comment"># 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP</span><br><span class="hljs-comment"># CONCURRENT_REQUESTS_PER_IP = 3</span><br> <br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment"># 8. 是否支持cookie，cookiejar进行操作cookie</span><br><span class="hljs-comment"># COOKIES_ENABLED = True</span><br><span class="hljs-comment"># COOKIES_DEBUG = True</span><br> <br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment"># 9. Telnet用于查看当前爬虫的信息，操作爬虫等...</span><br><span class="hljs-comment">#    使用telnet ip port ，然后通过命令操作</span><br><span class="hljs-comment"># TELNETCONSOLE_ENABLED = True</span><br><span class="hljs-comment"># TELNETCONSOLE_HOST = '127.0.0.1'</span><br><span class="hljs-comment"># TELNETCONSOLE_PORT = [6023,]</span><br> <br> <br><span class="hljs-comment"># 10. 默认请求头</span><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment"># DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span><br><span class="hljs-comment">#     'Accept-Language': 'en',</span><br><span class="hljs-comment"># &#125;</span><br> <br> <br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="hljs-comment"># 11. 定义pipeline处理请求</span><br><span class="hljs-comment"># ITEM_PIPELINES = &#123;</span><br><span class="hljs-comment">#    'step8_king.pipelines.JsonPipeline': 700,</span><br><span class="hljs-comment">#    'step8_king.pipelines.FilePipeline': 500,</span><br><span class="hljs-comment"># &#125;</span><br> <br> <br> <br><span class="hljs-comment"># 12. 自定义扩展，基于信号进行调用</span><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment"># EXTENSIONS = &#123;</span><br><span class="hljs-comment">#     # 'step8_king.extensions.MyExtension': 500,</span><br><span class="hljs-comment"># &#125;</span><br> <br> <br><span class="hljs-comment"># 13. 爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span><br><span class="hljs-comment"># DEPTH_LIMIT = 3</span><br> <br><span class="hljs-comment"># 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span><br> <br><span class="hljs-comment"># 后进先出，深度优先</span><br><span class="hljs-comment"># DEPTH_PRIORITY = 0</span><br><span class="hljs-comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'</span><br><span class="hljs-comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'</span><br><span class="hljs-comment"># 先进先出，广度优先</span><br> <br><span class="hljs-comment"># DEPTH_PRIORITY = 1</span><br><span class="hljs-comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'</span><br><span class="hljs-comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'</span><br> <br><span class="hljs-comment"># 15. 调度器队列</span><br><span class="hljs-comment"># SCHEDULER = 'scrapy.core.scheduler.Scheduler'</span><br><span class="hljs-comment"># from scrapy.core.scheduler import Scheduler</span><br> <br> <br><span class="hljs-comment"># 16. 访问URL去重</span><br><span class="hljs-comment"># DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'</span><br> <br> <br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span><br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">17. 自动限速算法</span><br><span class="hljs-string">    from scrapy.contrib.throttle import AutoThrottle</span><br><span class="hljs-string">    自动限速设置</span><br><span class="hljs-string">    1. 获取最小延迟 DOWNLOAD_DELAY</span><br><span class="hljs-string">    2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY</span><br><span class="hljs-string">    3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY</span><br><span class="hljs-string">    4. 当请求下载完成后，获取其"连接"时间 latency，即：请求连接到接受到响应头之间的时间</span><br><span class="hljs-string">    5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY</span><br><span class="hljs-string">    target_delay = latency / self.target_concurrency</span><br><span class="hljs-string">    new_delay = (slot.delay + target_delay) / 2.0 # 表示上一次的延迟时间</span><br><span class="hljs-string">    new_delay = max(target_delay, new_delay)</span><br><span class="hljs-string">    new_delay = min(max(self.mindelay, new_delay), self.maxdelay)</span><br><span class="hljs-string">    slot.delay = new_delay</span><br><span class="hljs-string">"""</span><br> <br><span class="hljs-comment"># 开始自动限速</span><br><span class="hljs-comment"># AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment"># 初始下载延迟</span><br><span class="hljs-comment"># AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment"># 最大下载延迟</span><br><span class="hljs-comment"># AUTOTHROTTLE_MAX_DELAY = 10</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to each remote server</span><br><span class="hljs-comment"># 平均每秒并发数</span><br><span class="hljs-comment"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br> <br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment"># 是否显示</span><br><span class="hljs-comment"># AUTOTHROTTLE_DEBUG = True</span><br> <br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br> <br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">18. 启用缓存</span><br><span class="hljs-string">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span><br><span class="hljs-string">    from scrapy.extensions.httpcache import DummyPolicy</span><br><span class="hljs-string">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span><br><span class="hljs-string">"""</span><br><span class="hljs-comment"># 是否启用缓存策略</span><br><span class="hljs-comment"># HTTPCACHE_ENABLED = True</span><br> <br><span class="hljs-comment"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span><br><span class="hljs-comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"</span><br><span class="hljs-comment"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span><br><span class="hljs-comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"</span><br> <br><span class="hljs-comment"># 缓存超时时间</span><br><span class="hljs-comment"># HTTPCACHE_EXPIRATION_SECS = 0</span><br> <br><span class="hljs-comment"># 缓存保存路径</span><br><span class="hljs-comment"># HTTPCACHE_DIR = 'httpcache'</span><br> <br><span class="hljs-comment"># 缓存忽略的Http状态码</span><br><span class="hljs-comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span><br> <br><span class="hljs-comment"># 缓存存储的插件</span><br><span class="hljs-comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span><br> <br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">19. 代理，需要在环境变量中设置</span><br><span class="hljs-string">    from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    方式一：使用默认</span><br><span class="hljs-string">        os.environ</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">            http_proxy:http://root:woshiniba@192.168.11.11:9999/</span><br><span class="hljs-string">            https_proxy:http://192.168.11.11:9999/</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">    方式二：使用自定义下载中间件</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    def to_bytes(text, encoding=None, errors='strict'):</span><br><span class="hljs-string">        if isinstance(text, bytes):</span><br><span class="hljs-string">            return text</span><br><span class="hljs-string">        if not isinstance(text, six.string_types):</span><br><span class="hljs-string">            raise TypeError('to_bytes must receive a unicode, str or bytes '</span><br><span class="hljs-string">                            'object, got %s' % type(text).__name__)</span><br><span class="hljs-string">        if encoding is None:</span><br><span class="hljs-string">            encoding = 'utf-8'</span><br><span class="hljs-string">        return text.encode(encoding, errors)</span><br><span class="hljs-string">        </span><br><span class="hljs-string">    class ProxyMiddleware(object):</span><br><span class="hljs-string">        def process_request(self, request, spider):</span><br><span class="hljs-string">            PROXIES = [</span><br><span class="hljs-string">                &#123;'ip_port': '111.11.228.75:80', 'user_pass': ''&#125;,</span><br><span class="hljs-string">                &#123;'ip_port': '120.198.243.22:80', 'user_pass': ''&#125;,</span><br><span class="hljs-string">                &#123;'ip_port': '111.8.60.9:8123', 'user_pass': ''&#125;,</span><br><span class="hljs-string">                &#123;'ip_port': '101.71.27.120:80', 'user_pass': ''&#125;,</span><br><span class="hljs-string">                &#123;'ip_port': '122.96.59.104:80', 'user_pass': ''&#125;,</span><br><span class="hljs-string">                &#123;'ip_port': '122.224.249.122:8088', 'user_pass': ''&#125;,</span><br><span class="hljs-string">            ]</span><br><span class="hljs-string">            proxy = random.choice(PROXIES)</span><br><span class="hljs-string">            if proxy['user_pass'] is not None:</span><br><span class="hljs-string">                request.meta['proxy'] = to_bytes（"http://%s" % proxy['ip_port']）</span><br><span class="hljs-string">                encoded_user_pass = base64.encodestring(to_bytes(proxy['user_pass']))</span><br><span class="hljs-string">                request.headers['Proxy-Authorization'] = to_bytes('Basic ' + encoded_user_pass)</span><br><span class="hljs-string">                print "**************ProxyMiddleware have pass************" + proxy['ip_port']</span><br><span class="hljs-string">            else:</span><br><span class="hljs-string">                print "**************ProxyMiddleware no pass************" + proxy['ip_port']</span><br><span class="hljs-string">                request.meta['proxy'] = to_bytes("http://%s" % proxy['ip_port'])</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="hljs-string">       'step8_king.middlewares.ProxyMiddleware': 500,</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    </span><br><span class="hljs-string">"""</span><br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">20. Https访问</span><br><span class="hljs-string">    Https访问时有两种情况：</span><br><span class="hljs-string">    1. 要爬取网站使用的可信任证书(默认支持)</span><br><span class="hljs-string">        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"</span><br><span class="hljs-string">        DOWNLOADER_CLIENTCONTEXTFACTORY = "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory"</span><br><span class="hljs-string">        </span><br><span class="hljs-string">    2. 要爬取网站使用的自定义证书</span><br><span class="hljs-string">        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"</span><br><span class="hljs-string">        DOWNLOADER_CLIENTCONTEXTFACTORY = "step8_king.https.MySSLFactory"</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        # https.py</span><br><span class="hljs-string">        from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory</span><br><span class="hljs-string">        from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate)</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        class MySSLFactory(ScrapyClientContextFactory):</span><br><span class="hljs-string">            def getCertificateOptions(self):</span><br><span class="hljs-string">                from OpenSSL import crypto</span><br><span class="hljs-string">                v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.key.unsecure', mode='r').read())</span><br><span class="hljs-string">                v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.pem', mode='r').read())</span><br><span class="hljs-string">                return CertificateOptions(</span><br><span class="hljs-string">                    privateKey=v1,  # pKey对象</span><br><span class="hljs-string">                    certificate=v2,  # X509对象</span><br><span class="hljs-string">                    verify=False,</span><br><span class="hljs-string">                    method=getattr(self, 'method', getattr(self, '_ssl_method', None))</span><br><span class="hljs-string">                )</span><br><span class="hljs-string">    其他：</span><br><span class="hljs-string">        相关类</span><br><span class="hljs-string">            scrapy.core.downloader.handlers.http.HttpDownloadHandler</span><br><span class="hljs-string">            scrapy.core.downloader.webclient.ScrapyHTTPClientFactory</span><br><span class="hljs-string">            scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</span><br><span class="hljs-string">        相关配置</span><br><span class="hljs-string">            DOWNLOADER_HTTPCLIENTFACTORY</span><br><span class="hljs-string">            DOWNLOADER_CLIENTCONTEXTFACTORY</span><br><span class="hljs-string">"""</span><br> <br> <br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">21. 爬虫中间件</span><br><span class="hljs-string">    class SpiderMiddleware(object):</span><br><span class="hljs-string">        def process_spider_input(self,response, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            下载完成，执行，然后交给parse处理</span><br><span class="hljs-string">            :param response: </span><br><span class="hljs-string">            :param spider: </span><br><span class="hljs-string">            :return: </span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            pass</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        def process_spider_output(self,response, result, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            spider处理完成，返回时调用</span><br><span class="hljs-string">            :param response:</span><br><span class="hljs-string">            :param result:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            return result</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        def process_spider_exception(self,response, exception, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            异常调用</span><br><span class="hljs-string">            :param response:</span><br><span class="hljs-string">            :param exception:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            return None</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    </span><br><span class="hljs-string">        def process_start_requests(self,start_requests, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            爬虫启动时调用</span><br><span class="hljs-string">            :param start_requests:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return: 包含 Request 对象的可迭代对象</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            return start_requests</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    内置爬虫中间件：</span><br><span class="hljs-string">        'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,</span><br><span class="hljs-string">        'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,</span><br><span class="hljs-string">        'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,</span><br><span class="hljs-string">        'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,</span><br><span class="hljs-string">        'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,</span><br><span class="hljs-string">"""</span><br><span class="hljs-comment"># from scrapy.contrib.spidermiddleware.referer import RefererMiddleware</span><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br>SPIDER_MIDDLEWARES = &#123;<br>   <span class="hljs-comment"># 'step8_king.middlewares.SpiderMiddleware': 543,</span><br>&#125;<br> <br> <br><span class="hljs-string">"""</span><br><span class="hljs-string">22. 下载中间件</span><br><span class="hljs-string">    class DownMiddleware1(object):</span><br><span class="hljs-string">        def process_request(self, request, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            请求需要被下载时，经过所有下载器中间件的process_request调用</span><br><span class="hljs-string">            :param request:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return:</span><br><span class="hljs-string">                None,继续后续中间件去下载；</span><br><span class="hljs-string">                Response对象，停止process_request的执行，开始执行process_response</span><br><span class="hljs-string">                Request对象，停止中间件的执行，将Request重新调度器</span><br><span class="hljs-string">                raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            pass</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    </span><br><span class="hljs-string">    </span><br><span class="hljs-string">        def process_response(self, request, response, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            spider处理完成，返回时调用</span><br><span class="hljs-string">            :param response:</span><br><span class="hljs-string">            :param result:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return:</span><br><span class="hljs-string">                Response 对象：转交给其他中间件process_response</span><br><span class="hljs-string">                Request 对象：停止中间件，request会被重新调度下载</span><br><span class="hljs-string">                raise IgnoreRequest 异常：调用Request.errback</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            print('response1')</span><br><span class="hljs-string">            return response</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        def process_exception(self, request, exception, spider):</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span><br><span class="hljs-string">            :param response:</span><br><span class="hljs-string">            :param exception:</span><br><span class="hljs-string">            :param spider:</span><br><span class="hljs-string">            :return:</span><br><span class="hljs-string">                None：继续交给后续中间件处理异常；</span><br><span class="hljs-string">                Response对象：停止后续process_exception方法</span><br><span class="hljs-string">                Request对象：停止中间件，request将会被重新调用下载</span><br><span class="hljs-string">            '''</span><br><span class="hljs-string">            return None</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    默认下载中间件</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,</span><br><span class="hljs-string">        'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">"""</span><br><span class="hljs-comment"># from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware</span><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment"># DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    'step8_king.middlewares.DownMiddleware1': 100,</span><br><span class="hljs-comment">#    'step8_king.middlewares.DownMiddleware2': 500,</span><br><span class="hljs-comment"># &#125;</span><br></code></pre></td></tr></table></figure>

<p>此文为转载<a href="https://www.cnblogs.com/wupeiqi/articles/6229292.html" target="_blank" rel="noopener">https://www.cnblogs.com/wupeiqi/articles/6229292.html</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Python/" class="category-chain-item">Python</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Python/" class="print-no-link">#Python</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>spider - Scrapy 爬虫</div>
      <div>https://flepeng.github.io/021-Python-36-Python-脚本-spider-spider-Scrapy-爬虫/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Lepeng</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2016年8月6日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/021-Python-36-Python-%E8%84%9A%E6%9C%AC-Python-%E8%BE%93%E5%87%BA%E5%9B%BA%E5%AE%9A%E9%95%BF%E5%BA%A6%E7%9A%84%E6%96%87%E6%9C%AC/" title="Python 输出固定长度的文本">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Python 输出固定长度的文本</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/021-Python-36-Python-%E8%84%9A%E6%9C%AC-spider-spider-%E6%89%8B%E5%86%99/" title="spider - 手写">
                        <span class="hidden-mobile">spider - 手写</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"flepeng/hexo-blog-comment","repo-id":"R_kgDOL0qaig","category":"Announcements","category-id":"DIC_kwDOL0qais4CfBIv","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
