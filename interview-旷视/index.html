

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2488174175014870" crossorigin="anonymous"></script><!-- google 广告 -->
  <meta name="google-site-verification" content="40lMg4eqLLbXoDcpN3h-cEnfmselbQ8tUzNvuC0IRIs" /><!-- google 站点认证 -->
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Lepeng">
  <meta name="keywords" content="">
  
    <meta name="description" content="one-stage two-stage 方法的优缺点faster rcnn 中 rpn 的实现过程one-stage 检测针对负样本太多 有什么解决办法 focal loss 随机丢弃负样本  ResNetResNet 的特点残差结构是为了解决网络退化的问题提出的(“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况。)，梯度消失&#x2F;爆炸已经通过 normalized initial">
<meta property="og:type" content="article">
<meta property="og:title" content="旷视">
<meta property="og:url" content="https://flepeng.github.io/interview-%E6%97%B7%E8%A7%86/index.html">
<meta property="og:site_name" content="Lepeng">
<meta property="og:description" content="one-stage two-stage 方法的优缺点faster rcnn 中 rpn 的实现过程one-stage 检测针对负样本太多 有什么解决办法 focal loss 随机丢弃负样本  ResNetResNet 的特点残差结构是为了解决网络退化的问题提出的(“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况。)，梯度消失&#x2F;爆炸已经通过 normalized initial">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-08-07T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-03T10:25:30.492Z">
<meta property="article:author" content="Feng Lepeng">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>旷视 - Lepeng</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"flepeng.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"f3d259b9efd9ce8655c180fd01bf0045","google":{"measurement_id":"G-LFTE4C7W3W"},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?f3d259b9efd9ce8655c180fd01bf0045";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-LFTE4C7W3W", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', 'G-LFTE4C7W3W');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Lepeng 的 blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="旷视"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-08-08 00:00" pubdate>
          2020年8月8日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          48 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">旷视</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="one-stage-two-stage-方法的优缺点"><a href="#one-stage-two-stage-方法的优缺点" class="headerlink" title="one-stage two-stage 方法的优缺点"></a>one-stage two-stage 方法的优缺点</h2><h2 id="faster-rcnn-中-rpn-的实现过程"><a href="#faster-rcnn-中-rpn-的实现过程" class="headerlink" title="faster rcnn 中 rpn 的实现过程"></a>faster rcnn 中 rpn 的实现过程</h2><h2 id="one-stage-检测针对负样本太多-有什么解决办法"><a href="#one-stage-检测针对负样本太多-有什么解决办法" class="headerlink" title="one-stage 检测针对负样本太多 有什么解决办法"></a>one-stage 检测针对负样本太多 有什么解决办法</h2><ul>
<li>focal loss</li>
<li>随机丢弃负样本</li>
</ul>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="ResNet-的特点"><a href="#ResNet-的特点" class="headerlink" title="ResNet 的特点"></a>ResNet 的特点</h3><p>残差结构是为了解决网络退化的问题提出的(“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况。)，梯度消失&#x2F;爆炸已经通过 normalized initialization 等方式得到解决。</p>
<p>将堆叠的几层layer称之为一个block，对于某个block，其可以拟合的函数为 F ( x ) F(x)F(x)，如果期望的潜在映射为H ( x ) H(x)H(x)，与其让F ( x ) F(x)F(x) 直接学习潜在的映射，不如去学习残差H ( x ) − x H(x)−xH(x)−x，即F ( x ) : &#x3D; H ( x ) − x F(x):&#x3D;H(x)−xF(x):&#x3D;H(x)−x，这样原本的前向路径上就变成了F ( x ) + x F(x)+xF(x)+x，用F ( x ) + x F(x)+xF(x)+x来拟合H ( x ) H(x)H(x)。作者认为这样可能更易于优化，因为相比于让F ( x ) F(x)F(x)学习成恒等映射，让F ( x ) F(x)F(x)学习成0要更加容易——后者通过L2正则就可以轻松实现。这样，对于冗余的block，只需F ( x ) → 0 F(x)→0F(x)→0就可以得到恒等映射，性能不减。</p>
<ol>
<li>学习结果对网络权重的波动变化更加敏感。</li>
<li>残差结果对数据的波动更加敏感。</li>
</ol>
<ul>
<li>与plain net相比，ResNet多了很多“旁路”，即shortcut路径，其首尾圈出的layers构成一个Residual Block；</li>
<li>ResNet中，所有的Residual Block都没有pooling层，降采样是通过conv的stride实现的；分别在conv3_1、conv4_1和conv5_1 Residual Block，降采样1倍，同时feature map数量增加1倍，如图中虚线划定的block；</li>
<li>通过Average Pooling得到最终的特征，而不是通过全连接层；</li>
<li>每个卷积层之后都紧接着BatchNorm layer，为了简化，图中并没有标出；</li>
</ul>
<p>观点1. 这也是孙剑的看法，即过于深的网络在反传时容易发生梯度弥散，一旦某一步开始导数小于1，此后继续反传，传到前面时，用float32位数字已经无法表示梯度的变化了，相当于梯度没有改变，也就是浅层的网络学不到东西了。这是网络太深反而效果下降的原因。加入ResNet中的shortcut结构之后，在反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度，这相当于把每一个block中向前传递的梯度人为加大了，也就会减小梯度弥散的可能性。</p>
<p>观点2.特征冗余：认为在正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。这显然会发生类似欠拟合的现象。加入shortcut结构，相当于在每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。</p>
<p>观点3.加入shortcut后相当于一个ensemble模型，输出的结果是前面各个block及其组合一起做的一个投票选出的结果。即可以把ResNet网络看成是多个子网络并行，从实验中看，真实起作用的路径长度并不深，主要走是中等深度的网络。简单来说，就是做了不同层次上的特征组合。</p>
<p>观点4.特征具有层次性的角度看：具有层次性，也就是说对于同一个任务，对不同样本可能适合不同层次（复杂程度）的特征就可以完成。例如：对一瓶纯净水和一瓶饮料做二分类，如果一个样本是黄色的，更具颜色特征判断它显然会是饮料，如果样本是无色的，就需要通过它的味道或成分等更复杂的特征判断了。回到网络结构上面，浅层网络提取的是简单的特征，而简单和复杂的特征适用于不同的样本，没有shortcut时，对所有样本的分类都是利用最复杂的特征判断，费时费力；加入shortcut后，相当于保留了一些简单的特征用于判断，变得省时。这一观点主要解释了为什么ResNet网络能够更快收敛。</p>
<p>观点5.从函数曲面的角度看：对于非常深的网络，其损失函数的图像是严重非凸的，很难找到一个较好的最优点。加入shortcut结构，使得损失函数的图像变得很接近凸函数，更容易（更快）找到较好最优点。（至于为什么shortcut能够使函数变凸，尚不理解…）</p>
<p>其中个人觉得观点1和2更为充分，其他观点更多的是从某一个角度去论证的，观点3和4最好结合在一起理解。</p>
<p>总的来说，我认为：由于每做一次卷积（包括对应的激活操作）都会浪费掉一些信息：比如卷积核参数的随机性（盲目性）、激活函数的抑制作用等等。这时，ResNet中的shortcut相当于把以前处理过的信息直接再拿到现在一并处理，起到了减损的效果。</p>
<h3 id="图像分类模型-ResNet101-相比-ResNet50-，residual-block-添加到了哪里，为什么要这样添加"><a href="#图像分类模型-ResNet101-相比-ResNet50-，residual-block-添加到了哪里，为什么要这样添加" class="headerlink" title="图像分类模型 ResNet101 相比 ResNet50 ，residual block 添加到了哪里，为什么要这样添加"></a>图像分类模型 ResNet101 相比 ResNet50 ，residual block 添加到了哪里，为什么要这样添加</h3><h2 id="one-stage-方法中-anchor-预测出的位置是有偏移量的，而类别置信度是针对原始位置的特征，并不是偏置后的特征，有什么办法"><a href="#one-stage-方法中-anchor-预测出的位置是有偏移量的，而类别置信度是针对原始位置的特征，并不是偏置后的特征，有什么办法" class="headerlink" title="one-stage 方法中 anchor 预测出的位置是有偏移量的，而类别置信度是针对原始位置的特征，并不是偏置后的特征，有什么办法"></a>one-stage 方法中 anchor 预测出的位置是有偏移量的，而类别置信度是针对原始位置的特征，并不是偏置后的特征，有什么办法</h2><ul>
<li>在预测一下置信度</li>
<li>使用计算偏移量的之后的框 计算 loss</li>
</ul>
<h2 id="模型很容易就过拟合怎么办？"><a href="#模型很容易就过拟合怎么办？" class="headerlink" title="模型很容易就过拟合怎么办？"></a>模型很容易就过拟合怎么办？</h2><ul>
<li>A: 加数据,数据是深度学习之友；</li>
<li>A：减小模型；</li>
<li>A: 正则化（L1、L2、Dropout）</li>
<li>A：BN；数据增强</li>
</ul>
<h2 id="输出值变成Nan了怎么办？"><a href="#输出值变成Nan了怎么办？" class="headerlink" title="输出值变成Nan了怎么办？"></a>输出值变成Nan了怎么办？</h2><p>损失函数相关：</p>
<ul>
<li>A：warm up learning rate; 可视化传入网络的数据，有可能是gt有问题</li>
<li>A：注意数值稳定性，尤其是exp()和log()之类的函数如log(0), 1&#x2F;0这种问题，需要仔细核验code</li>
</ul>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="梯度下降法和牛顿法区别"><a href="#梯度下降法和牛顿法区别" class="headerlink" title="梯度下降法和牛顿法区别"></a>梯度下降法和牛顿法区别</h3><p>梯度下降法：利用一阶导数<br>牛顿法：利用二阶导数，收敛速度快；但对目标函数有严格要求，必须有连续的一、二阶偏导数，计算量大</p>
<h2 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h2><h3 id="什么是BN-归一化"><a href="#什么是BN-归一化" class="headerlink" title="什么是BN 归一化"></a>什么是BN 归一化</h3><p>首先来说归一化的问题，神经网络训练开始前，都要对数据做一个归一化处理，归一化有很多好处，原因是网络学习的过程的本质就是学习数据分布，一旦训练数据和测试数据的分布不同，那么网络的泛化能力就会大大降低，另外一方面，每一批次的数据分布如果不相同的话，那么网络就要在每次迭代的时候都去适应不同的分布，这样会大大降低网络的训练速度，这也就是为什么要对数据做一个归一化预处理的原因。另外对图片进行归一化处理还可以处理光照，对比度等影响。</p>
<p>网络一旦训练起来，参数就要发生更新，出了输入层的数据外，其它层的数据分布是一直发生变化的，因为在训练的时候，网络参数的变化就会导致后面输入数据的分布变化，比如第二层输入，是由输入数据和第一层参数得到的，而第一层的参数随着训练一直变化，势必会引起第二层输入分布的改变，把这种改变称之为：Internal Covariate Shift，BN就是为了解决这个问题的。</p>
<p>和卷积层，激活层，全连接层一样，BN层也是属于网络中的一层。我们前面提到了，前面的层引起了数据分布的变化，这时候可能有一种思路是说：在每一层输入的时候，在加一个预处理多好。比如归一化到均值为0，方差为1，然后再送入输入进行学习。基本思路是这样的，然而实际上没有这么简单，如果我们只是使用简单的归一化方式：</p>
<p>对某一层的输入数据做归一化，然后送入网络的下一层，这样是会影响到本层网络所学习的特征的，比如网络中学习到的数据本来大部分分布在0的右边，经过RELU激活函数以后大部分会被激活，如果直接强制归一化，那么就会有大多数的数据无法激活了，这样学习到的特征不就被破坏掉了么？论文中对上面的方法做了一些改进：变换重构，引入了可以学习的参数，这就是算法的关键之处：这两个希腊字母就是要学习的。</p>
<p><a href="https://www.jianshu.com/p/fcc056c1c200" target="_blank" rel="noopener">https://www.jianshu.com/p/fcc056c1c200</a></p>
<h3 id="BN-的作用"><a href="#BN-的作用" class="headerlink" title="BN 的作用"></a>BN 的作用</h3><p>加快网络的训练和收敛的速度<br>控制梯度爆炸防止梯度消失<br>防止过拟合</p>
<p>分析：<br>（1）加快收敛速度：在深度神经网络中中，如果每层的数据分布都不一样的话，将会导致网络非常难收敛和训练，而如果把 每层的数据都在转换在均值为零，方差为1 的状态下，这样每层数据的分布都是一样的训练会比较容易收敛。<br>（2）防止梯度爆炸和梯度消失：<br>梯度消失：在深度神经网络中，如果网络的激活输出很大，其对应的梯度就会很小，导致网络的学习速率就会很慢，假设网络中每层的学习梯度都小于最大值0.25，网络中有n层，因为链式求导的原因，第一层的梯度将会小于0.25的n次方，所以学习速率相对来说会变的很慢，而对于网络的最后一层只需要对自身求导一次，梯度就大，学习速率就会比较快，这就会造成在一个很深的网络中，浅层基本不学习，权值变化小，而后面几层网络一直学习，后面的网络基本可以表征整个网络，这样失去了深度的意义。（使用BN层归一化后，网络的输出就不会很大，梯度就不会很小）<br>梯度爆炸：第一层偏移量的梯度&#x3D;激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n，假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加。（使用bn层后权值的更新也不会很大）<br>（3）BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝这一个方向使劲学习。一定程度上避免了过拟合。<br><a href="https://zhuanlan.zhihu.com/p/75603087" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75603087</a></p>
<h3 id="为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后"><a href="#为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后" class="headerlink" title="为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后"></a>为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后</h3><p>因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移，相反的，全连接和卷积层的输出一般是一个对称,非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。其实想想也是的，像relu这样的激活函数，如果你输入的数据是一个高斯分布，经过他变换出来的数据能是一个什么形状？小于0的被抑制了，也就是分布小于0的部分直接变成0了，这样不是很高斯了</p>
<h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><p><strong>激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。解决线性模型所不能解决的问题</strong></p>
<p>如果不用激活函数，在这种情况下每一层输出都是上层输入的线性函数。容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。因此引入非线性函数作为激活函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。</p>
<h3 id="引入ReLu的原因"><a href="#引入ReLu的原因" class="headerlink" title="引入ReLu的原因"></a>引入ReLu的原因</h3><p>第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。</p>
<p>第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现 梯度消失 的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。</p>
<p>第三，ReLu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生</p>
<h2 id="哪些原因会导致梯度消失。答：网络深度、激活函数"><a href="#哪些原因会导致梯度消失。答：网络深度、激活函数" class="headerlink" title="哪些原因会导致梯度消失。答：网络深度、激活函数"></a>哪些原因会导致梯度消失。答：网络深度、激活函数</h2><h2 id="如何处理大小不同的图片输入（全卷积网络）"><a href="#如何处理大小不同的图片输入（全卷积网络）" class="headerlink" title="如何处理大小不同的图片输入（全卷积网络）"></a>如何处理大小不同的图片输入（全卷积网络）</h2><ol>
<li>从图像数据入手，最简单最粗暴的方法就是resize到指定大小，虽然简单粗暴，但是有效。但是这个也要因任务而异，比如普通的图像分类问题，resize一下可能无碍，然而物体检测时物体发生了形变，可能就会很影响效果，这时候需要使用更加精细的resize手段。或者你可以crop特定位置的图像区域，这样需要一定的额外算法或者人工的辅助，操作起来不如resize。</li>
<li>从模型入手，比如物体检测中使用的SPP-Net，取消了全连接层的设计，就可以支持任意大小输入。事实上，全连接层是制约输入大小的关键因素，因为卷积和池化层根本不care你输入尺寸是多少，他们只管拿到前一层的feature map，然后做卷积池化输出就好了，只有全连接层，因为权重维度固定了，就不能改了，这样层层向回看，才导致了所有的尺寸都必须固定才可以。</li>
</ol>
<h2 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h2><ul>
<li>交叉熵损失函数</li>
<li>平方差损失函数</li>
<li>绝对值损失函数</li>
</ul>
<h2 id="如何检测小物体等"><a href="#如何检测小物体等" class="headerlink" title="如何检测小物体等"></a>如何检测小物体等</h2><ul>
<li>提高图像拍摄分辨率</li>
<li>增加模型的输入分辨率</li>
<li>降低下采样率与空洞卷积，减少小物体特征损失，同时在不改变网络分辨率的前提下增加网络的感受野，减小对大物体检测性能的损失</li>
<li>多尺度训练（Multi Scale Training, MST）通常是指设置几种不同的图片输入尺度，训练时从多个尺度中随机选取一种尺度，将输入图片缩放到该尺度并送入网络中，是一种十分有效的trick方法，推理时可对输入图片进行放大，用以针对小物体检测</li>
<li>优化Anchor尺寸设计</li>
</ul>
<h2 id="Focal-loss-的2个参数有什么作用"><a href="#Focal-loss-的2个参数有什么作用" class="headerlink" title="Focal loss 的2个参数有什么作用"></a>Focal loss 的2个参数有什么作用</h2><ul>
<li><p>添加参数γ，当γ大于0时，对于易分的正样本或负样本，权重小，而对于难区分的样本则权重大，避免让简单样本主导loss，γ越大，困难样本的权重越大</p>
</li>
<li><p>添加参数α，用来平衡正负样本本身的比例不均</p>
</li>
</ul>
<h2 id="FPN为什么能提升小目标的准确率"><a href="#FPN为什么能提升小目标的准确率" class="headerlink" title="FPN为什么能提升小目标的准确率"></a>FPN为什么能提升小目标的准确率</h2><ul>
<li>FPN可以利用经过top-down模型后的那些上下文信息（高层语义信息）；</li>
<li>对于小目标而言，FPN增加了特征映射的分辨率（即在更大的feature map上面进行操作，这样可以获得更多关于小目标的有用信息），如图中所示；</li>
</ul>
<h2 id="如何提升检测的Recall-Precision"><a href="#如何提升检测的Recall-Precision" class="headerlink" title="如何提升检测的Recall, Precision"></a>如何提升检测的Recall, Precision</h2><p>从算法层面：</p>
<ol>
<li>对于想提高 Precision，使得二分类器预测的正例尽可能是真实正例，那么应该提高二分类器预测正例的阈值，使得预测出来的结果尽可能是正确的；</li>
<li>对于想提高 Recall，使得二分类器尽可能将真实的正例挑选出来，那么应该降低二分类器预测正例阈值，使得预测出来的结果尽可能包含更多的正样本，即可以简单调整socre的阈值。</li>
</ol>
<p>从数据层面：</p>
<ol>
<li>对于想提高Precision，无非是你的模型对于某些样本检测错误，理解为对某些样本的拟合能力不足，那么在训练过程中做好(在线&#x2F;离线)数据增强，让模型有足够的泛化能力，即可提高Precision；</li>
<li>对于想提高Recall，无非是有些样本没有正确检测出来，即难样本过多，可以进行在线难例挖掘和挑出难例样本继续训练以提高模型能力。 (注意这里的检测错误和没有检测出来是两个概念)</li>
</ol>
<h3 id="检测任务中precsion低recall高可能是什么原因？"><a href="#检测任务中precsion低recall高可能是什么原因？" class="headerlink" title="检测任务中precsion低recall高可能是什么原因？"></a>检测任务中precsion低recall高可能是什么原因？</h3><p>A：可能是label assign过程正样本卡的阈值太低，以至于有很多低质量正样本所致。同理，如果precision很高recall低，则表示正样本卡的阈值太高，以至于有很多高质量正样本被忽略或者设为负样本</p>
<h2 id="如果两个人的检测框重叠部分过大应该如何把两个人分开"><a href="#如果两个人的检测框重叠部分过大应该如何把两个人分开" class="headerlink" title="如果两个人的检测框重叠部分过大应该如何把两个人分开"></a>如果两个人的检测框重叠部分过大应该如何把两个人分开</h2><ol>
<li>目前一般采用soft-nms。但是这个东西有人说好也有人说没毛用。</li>
<li>做crop数据增强，这样能对遮挡识别有利。（参考SSD）</li>
<li>.旷世今年的repulsion loss:让proposal尽量远离和他overlap第二大的GT，同时也让不同GT上的proposals之间相互远离。我们目前测试这个方法确实很有效</li>
</ol>
<h2 id="说一说SSD具体是怎么操作的"><a href="#说一说SSD具体是怎么操作的" class="headerlink" title="说一说SSD具体是怎么操作的"></a>说一说SSD具体是怎么操作的</h2><h2 id="其他一些问题"><a href="#其他一些问题" class="headerlink" title="其他一些问题"></a>其他一些问题</h2><ol>
<li><p>FCN结构介绍，上采样的具体操作<br>  全卷积神经网络，主要用于语义分割， 上采样采用 最近邻法，双线性插值。</p>
</li>
<li><p>如果要在某硬件上部署resnet，7x7卷积上不支持怎么办？shortcut不支持怎么办？1x1卷积不支持怎么办？depthwise卷积不支持怎么办？因为硬件原因，add的时候channel数对不上怎么办？需要做一些low level的任务，输入图片特别大，怎么减少访存时间？<br>  当时答的也不好，下来之后也思考了一下，感觉可以从卷积等效替换的角度和RepVGG的重参数化角度来回答的，比如一个7x7可以用3个3x3等效替换，shortcut可以用RepVGG的思路进行重参数化，channel数对不上可以padding或者删除，1x1不支持可以使用老版本的ResNet，每个block都是由两个3x3卷积组成，depthwise卷积不支持，可以和后面的pointwise卷积融合成一个普通卷积(当然有额外的开销，甚至于如果depthwise后面万一跟了relu，还不能这么做)，low level的任务输入图片特别大，可以尝试分块计算，而不是一次性把数据全load完再计算（不过他要求的是从模型设计的角度来考虑，分块的思路应该是硬件实现的角度，如果把异构计算牵扯进来上面问题都能回答，但并不是我的领域），这块确实答的不好，后续还需要继续去寻找更正确的答案……</p>
</li>
</ol>
<h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><h3 id="GPU较于CPU加速"><a href="#GPU较于CPU加速" class="headerlink" title="GPU较于CPU加速"></a>GPU较于CPU加速</h3><p>在训练网络中，其实大量的运算资源都消耗在了数值计算上面，大部分网络训练的过程都是1.计算loss，2.根据loss求梯度，3.再根据梯度更新参数（梯度下降原理）。无论在GPU还是CPU中，都是不断重复123步。但是由于CPU是通用计算单元（并不擅长数值运行），而GPU特长是图像处理（数值计算）。所以GPU更加适合训练网络，从而起到加速效果。</p>
<p>GPU起初的设计目标就是为了处理这种图形图像的渲染工作，而这种工作的特性就是可以分布式、每个处理单元之间较为独立，没有太多的关联。而一部分机器学习算法，比如遗传算法，神经网络等，也具有这种分布式及局部独立的特性（e.g.比如说一条神经网络中的链路跟另一条链路之间是同时进行计算，而且相互之间没有依赖的），这种情况下可以采用大量小核心同时运算的方式来加快运算速度</p>
<h3 id="为什么使用一个更大的GPU比使用两个并行的GPU-效果好"><a href="#为什么使用一个更大的GPU比使用两个并行的GPU-效果好" class="headerlink" title="为什么使用一个更大的GPU比使用两个并行的GPU 效果好"></a>为什么使用一个更大的GPU比使用两个并行的GPU 效果好</h3><p>假设一台机器上有k块GPU。给定需要训练的模型，每块GPU及其相应的显存将分别独立维护一份完整的模型参数。在模型训练的任意一次迭代中，给定一个随机小批量，我们将该批量中的样本划分成k份并分给每块显卡的显存一份。然后，每块GPU将根据相应显存所分到的小批量子集和所维护的模型参数分别计算模型参数的本地梯度。接下来，我们把k块显卡的显存上的本地梯度相加，便得到当前的小批量随机梯度。之后，每块GPU都使用这个小批量随机梯度分别更新相应显存所维护的那一份完整的模型参数。</p>
<p>可以发现，除了计算之外，多块 GPU 之间还要相互通信，所以一个更大的GPU比使用两个并行的GPU 效果好</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%AE%97%E6%B3%95/" class="print-no-link">#算法</a>
      
        <a href="/tags/%E9%9D%A2%E8%AF%95/" class="print-no-link">#面试</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>旷视</div>
      <div>https://flepeng.github.io/interview-旷视/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Lepeng</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2020年8月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"flepeng/hexo-blog-comment","repo-id":"R_kgDOL0qaig","category":"Announcements","category-id":"DIC_kwDOL0qais4CfBIv","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
